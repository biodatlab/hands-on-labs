# Homework / Discussion

In this class, you've learned how to set up Ollama, download an open-source model, and perform summarization tasks with an LLM. To deepen your understanding, explore the following questions:

1. Tokenization in LLMs:
   - Describe the process of tokenization in LLMs.
   - Why is byte-pair encoding (BPE) commonly used? What are its advantages and limitations?
   - Practical task: Use `tiktoken` to count the tokens using GPT-4o tokenizer in the Thai sentence "ฉันอยากกินข้าว" and English sentence "I wanna eat.".
   - Discuss why using GPT-4o might be more expensive when prompting in Thai.

2. Text Generation Parameters:
   - Explain the role of temperature and top-k/top-p sampling in text generation with LLMs.
   - How do these parameters affect the output?
   - In what scenarios might you adjust these parameters?

3. Retrieval Augmented Generation (RAG):
   - Discuss what Retrieval Augmented Generation (RAG) is.
   - Why do you think RAG is useful for LLMs?
   - Can you think of any potential applications where RAG would be particularly beneficial?

4. LLM Leaderboard Analysis:
   - Visit the [Chatbot Arena](https://lmarena.ai/) leaderboard.
   - Analyze the current standings up to the `Meta-Llama-3.1` model, considering:
     - Scores
     - Licenses
     - Knowledge cutoff dates
   - What trends or insights can you draw from this analysis?

5. Hallucinations in LLMs:
   - Explain the technical reasons why LLMs sometimes produce factually incorrect information (hallucinations).
   - Research and discuss current directions being explored to mitigate this issue.
   - How might these mitigation strategies impact the use of LLMs in various applications?

For each question, write a brief discussion (about 2 paragraphs).