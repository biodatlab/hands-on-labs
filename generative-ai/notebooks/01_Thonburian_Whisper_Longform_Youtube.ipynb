{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBjKiLi_oxxC"
      },
      "source": [
        "## **Long-form Transcription with Thonburian Whisper**\n",
        "\n",
        "This Jupyter notebook demonstrates the process of performing long-form transcription using Thonburian Whisper, a specialized model for Thai language speech recognition. Thonburian Whisper is an adaptation of OpenAI's Whisper model, fine-tuned on Thai speech data to improve accuracy for Thai language transcription tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibbBX6FxoeON"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install attacut\n",
        "!pip install ssg\n",
        "!pip install datasets\n",
        "!pip install pyarrow==15.0.2\n",
        "!pip install pydub\n",
        "!pip install ipywebrtc\n",
        "!pip install openai\n",
        "!pip install yt-dlp --upgrade\n",
        "!pip install gradio==3.44.3\n",
        "!pip install pandas==2.2.2 # for numpy 1.26.4 & datasets module compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DAigVlqAo_no"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/biodatlab/thonburian-whisper/\n",
        "!cp ./thonburian-whisper/longform_transcription/sentence_segment.py .\n",
        "!cp ./thonburian-whisper/longform_transcription/utils.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GzKpdLu-ngIL"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade numpy # https://www.geeksforgeeks.org/python/how-to-fix-valueerror-numpy-ndarray-size-changed-in-pyxdameraulevenshtein-package/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import yt_dlp\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Audio, Dataset\n",
        "from transformers import pipeline\n",
        "from sentence_segment import SyllableSegmentation\n",
        "from utils import convert_mp4_to_wav, perform_vad, generate_srt, burn_srt_to_video\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9J42MOnBtgFf"
      },
      "outputs": [],
      "source": [
        "class LongformTranscriber:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sr: int = 16000,\n",
        "        model_path: str = \"biodatlab/whisper-th-medium-combined\",\n",
        "        chunk_length_s: int = 30,\n",
        "        batch_size: int = 4,\n",
        "        language: str = \"th\",\n",
        "        segment_duration: float = 4.0\n",
        "    ):\n",
        "        self.sr = sr\n",
        "        self.model_path = model_path\n",
        "        self.chunk_length_s = chunk_length_s\n",
        "        self.batch_size = batch_size\n",
        "        self.language = language\n",
        "        self.segment_duration = segment_duration\n",
        "\n",
        "        # Initialize ASR pipeline\n",
        "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.pipe = pipeline(\n",
        "            \"automatic-speech-recognition\",\n",
        "            model=self.model_path,\n",
        "            chunk_length_s=self.chunk_length_s,\n",
        "            device=device,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        self.ss = SyllableSegmentation()\n",
        "\n",
        "    def convert_audio_to_wav(self, audio_file, target_sr):\n",
        "        audio = AudioSegment.from_file(audio_file)\n",
        "        audio = audio.set_frame_rate(target_sr).set_channels(1)\n",
        "        output_wav_file = audio_file.rsplit('.', 1)[0] + \"_converted.wav\"\n",
        "        audio.export(output_wav_file, format=\"wav\")\n",
        "        return output_wav_file\n",
        "\n",
        "    def transcribe(self, audio_path: str):\n",
        "        if audio_path.endswith('.mp4'):\n",
        "            wav_file = self.convert_mp4_to_wav(audio_path)\n",
        "        elif audio_path.endswith('.wav'):\n",
        "            # Check sampling rate and convert if necessary\n",
        "            audio = AudioSegment.from_wav(audio_path)\n",
        "            if audio.frame_rate != self.sr:\n",
        "                wav_file = self.convert_audio_to_wav(audio_path, self.sr)\n",
        "            else:\n",
        "                wav_file = audio_path\n",
        "        else:  # Assuming other audio formats such as .mp3, etc.\n",
        "            wav_file = self.convert_audio_to_wav(audio_path, self.sr)\n",
        "\n",
        "        _, chunklist = perform_vad(wav_file, 'temp_directory_for_chunks')\n",
        "        print((chunklist))\n",
        "        # for faster inference, create dataset and feed to prediction pipeline\n",
        "        # audio_dataset = Dataset.from_dict({\"audio\": [c[\"fname\"] for c in chunklist]}).cast_column(\"audio\", Audio()) # deprecated ?\n",
        "        audio_dataset = [c[\"fname\"] for c in chunklist]\n",
        "        # print(type(audio_dataset))\n",
        "        # print(audio_dataset)\n",
        "\n",
        "        prediction_gen = self.pipe(\n",
        "            # audio_dataset[\"audio\"], # deprecated ?\n",
        "            audio_dataset[0:3], # test with sample wav\n",
        "            generate_kwargs={\"task\": \"transcribe\", \"language\": self.language},\n",
        "            return_timestamps=False,\n",
        "            batch_size=self.batch_size,\n",
        "        )\n",
        "\n",
        "        predictions = [out for out in prediction_gen]\n",
        "        vad_transcriptions = {\n",
        "            \"start\": [(chunk[\"start\"] / self.sr) for chunk in chunklist],\n",
        "            \"end\": [(chunk[\"end\"] / self.sr) for chunk in chunklist],\n",
        "            \"prediction\": [pred[\"text\"] for pred in predictions]\n",
        "        }\n",
        "        uncorrected_segments = self.ss(vad_transcriptions=vad_transcriptions, segment_duration=self.segment_duration)\n",
        "        return uncorrected_segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88_-gIUfuR6R"
      },
      "source": [
        "Create `LongformTranscriber` and transcribe `audio.mp3` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcriber = LongformTranscriber(\n",
        "    sr=16000,\n",
        "    model_path=\"biodatlab/whisper-th-medium-combined\",\n",
        "    chunk_length_s=30,\n",
        "    batch_size=4,\n",
        "    language=\"th\",\n",
        "    segment_duration=4.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "R8JQr6SWKgpr"
      },
      "outputs": [],
      "source": [
        "def _return_yt_html_embed(yt_url):\n",
        "    video_id = yt_url.split(\"?v=\")[-1]\n",
        "    HTML_str = (\n",
        "        f'<center> <iframe width=\"500\" height=\"320\" src=\"https://www.youtube.com/embed/{video_id}\"> </iframe>'\n",
        "        \" </center>\"\n",
        "    )\n",
        "    return HTML_str\n",
        "\n",
        "\n",
        "def yt_transcribe(yt_url: str):\n",
        "    # try:\n",
        "      ydl_opts = {\n",
        "          'format': 'bestaudio/best',\n",
        "          'postprocessors': [{\n",
        "              'key': 'FFmpegExtractAudio',\n",
        "              'preferredcodec': 'mp3',\n",
        "              'preferredquality': '192',\n",
        "          }],\n",
        "          'outtmpl': 'audio.%(ext)s',\n",
        "      }\n",
        "      with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "          info = ydl.extract_info(yt_url, download=True)\n",
        "          video_id = info['id']\n",
        "\n",
        "      html_embed_str = _return_yt_html_embed(video_id)\n",
        "      print('transcribing')\n",
        "      transcripts = transcriber.transcribe(\"audio.mp3\")\n",
        "      transcripts = pd.DataFrame(transcripts)  # Convert to DataFrame\n",
        "\n",
        "      # Clean up the downloaded file\n",
        "      os.remove(\"audio.mp3\")\n",
        "\n",
        "      return html_embed_str, transcripts\n",
        "    # except Exception as e:\n",
        "        # return f\"Error: {str(e)}\", \"An error occurred while processing the YouTube video.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, transcripts = yt_transcribe(\"https://www.youtube.com/watch?v=TtBD1kkmRqw\")\n",
        "print(transcripts)\n",
        "# pd.DataFrame(transcripts).to_csv(\"content.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
