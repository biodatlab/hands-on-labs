{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1df3e0",
   "metadata": {},
   "source": [
    "## Connect Ollama with Langchain\n",
    "\n",
    "Running `Ollama` in the backend on your local computer and connect with Python using Langchain.\n",
    "\n",
    "Make sure the command `ollama run llama3.1` is already running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain\n",
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42f4860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's work through this step by step:\n",
      "\n",
      "You started with 10 apples.\n",
      "\n",
      "You gave 21 apples to your student, but wait... you can't give away more apples than you have! It seems like there might be a mistake. Let's assume that was a typo and you meant to say you gave some number of apples to the student (which is less than or equal to 10).\n",
      "\n",
      "Let's call the correct number of apples given to the student \"x\". So, after giving x apples to the student, you have:\n",
      "\n",
      "10 - x apples left\n",
      "\n",
      "Then, you ate 1 apple. So, subtract 1 from that amount:\n",
      "\n",
      "(10 - x) - 1 = (10 - x - 1)\n",
      "\n",
      "To answer the question, we need to know how many apples you gave away. If you can tell me what number \"x\" is, I'd be happy to help!\n"
     ]
    }
   ],
   "source": [
    "# Code from https://stackoverflow.com/a/78430197/3626961\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain import PromptTemplate # Added\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\", stop=[\"<|eot_id|>\"]) # Added stop token\n",
    "\n",
    "def get_model_response(user_prompt, system_prompt):\n",
    "    # NOTE: No f string and no whitespace in curly braces\n",
    "    template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {user_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "    # Added prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Modified invoking the model\n",
    "    response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example\n",
    "user_prompt = \"I have 10 apples. I give 21 apples to my student and ate 1 apple. How many apples I have left?\"\n",
    "system_prompt = \"You are a helpful assistant doing as the given prompt.\"\n",
    "print(get_model_response(user_prompt, system_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodat-lab-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
